{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Research Question\n",
    "How efficient does the first price sealed bid auction allocate goods between ZI-C agents and reinforcement learning agents?\n",
    "\n",
    "The research question is interesting and important because auction is an alternative market structure to formal price setting structure. One of the most common auction format is first price sealed bid. \n",
    "\n",
    "## Brief Review\n",
    "\n",
    "\n",
    "## Hypothesis\n",
    "Since ZI-C agents cannot learn but RL agents can, on average, the market with RL agents will allocate goods more efficiently than the ZI-C agents as auctions are repeated over time with the same set of agents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Design\n",
    "* Agents:\n",
    "    * General assumptions:\n",
    "    For each agent $i \\in [1,101]$:\n",
    "        * The private value $V_i \\in [0,100]$, and $V_i \\neq V_j \\forall i,j \\in N$.\n",
    "        * The strategy set $S_i = \\{ 0,1,2,...,100 \\}$ is possible bids $b_i \\in S_i$ that agent can submit\n",
    "        * The payoff for each bid $b_i$ submitted: \n",
    "         $P(b_i)$ = \\begin{cases} \n",
    "      V_i - b_i &  b_i \\geq \\max\\{b_j\\} \\forall j \\in N \\\\\n",
    "      0 &  b_i < \\max\\{b_j\\} \\forall j \\in N \n",
    "           \\end{cases} \n",
    "    * ZI-C agents:\n",
    "        * Agent is constrained to submit $b_i \\in [0,V_i]$\n",
    "        * The bid $b_i$ ~ $U[0,V_i]$\n",
    "    * Reinforced Learning (RL) agents:\n",
    "        * Agent learns over strategies $b_i \\in S_i$. Each strategy is assigned an initial value $P_0(b_i) = \\underline{v},\\forall b_i \\in S_i$ with $\\underline{v} = 0$ is the lowest payoff possible in the first price sealed bid auction format with no entrance fee.\n",
    "        * Before each auction, agent submit a bid $b_i \\in S_i$. The bid is decided by the policy function (strategy selection rule) which is logit choice function (Boltmanz or softmax): $Pr(b_i) = \\frac{e^{\\lambda P(b_i)} }{\\sum_{b\\in S_i} e^{\\lambda P(b)}}$\n",
    "        * Reinforcement rule is standard $P_{t+1}(b_i) = (1-\\alpha)P_t(b_i) + \\alpha x$ with $x$ is the payoff from player i plays $b_i \\in S_i$ in auction at period t. The rule is exponential recency-weighted average rule with $\\alpha$ determines forgetting. (Roshoka & Romero, 2019) Value of other strategies will remain the same.\n",
    "* Environment:\n",
    "    * First price sealed bid auctions\n",
    "    * Each auction has N = 101 agents participated\n",
    "    * The item is rewarded to the bidder with the highest bid submitted\n",
    "    * The auction reruns with the same set of agents for T = 5000 rounds.\n",
    "    * The allocative efficiency is calculated after each 100 rounds (1 supergame = 100 rounds) then the dynamic efficiency of 2 markets between ZI-C agents and RL agents can be compared over time. We know ZI-C agents do not learn over games and supergames, but RL agents do. Hence, as auctions repeat over time with two types of agents, the efficiency of RL agent-based market will exhibit improvement while efficiency of ZI-C agent-based market should remain on average the same.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
